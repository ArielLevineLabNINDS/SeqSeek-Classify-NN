{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2QqIAAKCtZT"
   },
   "source": [
    "# Running the secondary model\n",
    "\n",
    "This model was designed to be run after label transfer.  It uses log+1 normalized counts for cells that are labelled as \n",
    "neurons (or doublets)\n",
    "\n",
    "Before running, you need to make sure that you have the packages listed in the imports below.  \n",
    "Also, I am running tensorflow 2.2.\n",
    "\n",
    "# step one\n",
    "### import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dGsGtW-CtZV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import sklearn.preprocessing\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jWTSIQRCtZf"
   },
   "source": [
    "### define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFBK2uRuCtZk"
   },
   "outputs": [],
   "source": [
    "## convert the sparse matrix into a log x+1 normalized sparse tensor.\n",
    "def make_log_maxn_tensor(sparseMatrix,ds_list,ds_type):\n",
    "  # filter the sparse matrix... (note: cannot be a COO formatted matrix)\n",
    "  indx = np.equal( np.array(ds_list,dtype=object),np.array(ds_type,dtype=object) )\n",
    "  x0 = sparseMatrix[indx,:]\n",
    "  del indx\n",
    "\n",
    "  # get the log transform...\n",
    "  x0.data = np.log1p(x0.data)\n",
    "  # normalize to the max...\n",
    "  sklearn.preprocessing.normalize(x0, norm=\"max\", axis=1, copy=False)\n",
    "\n",
    "  # convert the filtered matrix to COO format\n",
    "  x0 = x0.tocoo()\n",
    "  indices = np.mat([x0.row, x0.col]).transpose()\n",
    "\n",
    "  # make a sparse tensor and re-order it...\n",
    "  x0 = tf.SparseTensor(indices,x0.data,x0.shape)\n",
    "  x0 = tf.sparse.reorder(x0)\n",
    "  return x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Opo0KsGxrqrt"
   },
   "source": [
    "Copy the data from github to the VM.  We need to install git-lfs, because the count matrix is too large to be placed in github, so we use the git lfs (large file system. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "_kJReHPCszVs",
    "outputId": "7e067254-7037-4b66-9dd4-b85554bfb127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.3.4-1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-440\n",
      "Use 'sudo apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get -q install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "gq_a06git3rK",
    "outputId": "71f622d0-7e64-4ed0-fe2a-c47263dc3165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
      "          with new flags from 'git clone'\n",
      "\n",
      "'git clone' has been updated in upstream Git to have comparable\n",
      "speeds to 'git lfs clone'.\n",
      "Cloning into 'SeqSeek-Classify-NN'...\n",
      "remote: Enumerating objects: 20, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 20 (delta 2), reused 16 (delta 1), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (20/20), done.\n",
      "Git LFS: (1 of 1 files) 603.08 MB / 603.08 MB\n",
      "%%MatrixMarket matrix coordinate integer general\n",
      "28238 25419 47542555\n",
      "1 1 1\n",
      "3 1 3\n",
      "4 1 13\n",
      "5 1 9\n",
      "7 1 1\n",
      "9 1 2\n",
      "10 1 13\n",
      "12 1 2\n"
     ]
    }
   ],
   "source": [
    "! rm -rf SeqSeek-Classify-NN\n",
    "! git lfs clone https://github.com/ArielLevineLabNINDS/SeqSeek-Classify-NN.git\n",
    "! head SeqSeek-Classify-NN/filtered_neurons_doublets.mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxQpbfTuCtZr"
   },
   "source": [
    "# step two: \n",
    "### data loading...\n",
    "\n",
    "We are going to re-create the test data from the analysis.  In the future, I will create a \n",
    "dataset that is only test data, then I will try to create a function that will convert your data\n",
    "into the appropriat format.  After all, it's more fun to play with your data.  Keep in mind that\n",
    "this data is mouse spinal cord neurons and doublets only.  \n",
    "\n",
    "The input data is a sparse matrix created in R from Seurat data.  In Seurat, the count matrices are genes (rows) by cells (columns).  \n",
    "Tensorflow requires cells (rows) by genes (columns).  I performed the matrix transpose in R, so you won't see it here.\n",
    "\n",
    "note: it takes just over a minute to read the data on my laptop (macbook pro 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "D5hZaN1ECtZt",
    "outputId": "1231b185-d10c-4f8c-c6df-f69b0fb32038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to read data files: 0.071 s\n",
      "all cell_ids from the labels == cell_ids from matrix: True\n",
      "time to read matrix: 96.645 s\n",
      "converting raw counts to CSR\n",
      "time to convert matrix: 1.004 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "genes = pd.read_csv(\"SeqSeek-Classify-NN/filtered_neurons_doublets_genes.tsv\",names=[\"gene\"])\n",
    "df = pd.read_csv(\"SeqSeek-Classify-NN/filtered_neurons_doublets_barcodes.tsv\",names=[\"cell_id\"])\n",
    "lbl = pd.read_csv(\"SeqSeek-Classify-NN/filtered_neurons_doublets_labels.csv\")\n",
    "t1 = time.time()\n",
    "print(f\"time to read data files: {t1-t0:.3f} s\")\n",
    "print(f\"all cell_ids from the labels == cell_ids from matrix: {all(df.cell_id == lbl.cell_id)}\")\n",
    "\n",
    "t0 = time.time()\n",
    "dirty_neurons = scipy.io.mmread(\"SeqSeek-Classify-NN/filtered_neurons_doublets.mtx\")\n",
    "t1 = time.time()\n",
    "print(f\"time to read matrix: {t1-t0:.3f} s\")\n",
    "if scipy.sparse.isspmatrix_coo(dirty_neurons):\n",
    "  print(\"converting raw counts to CSR\")\n",
    "  t0 = time.time()\n",
    "  dirty_neurons = dirty_neurons.tocsr()\n",
    "  t1 = time.time()\n",
    "  print(f\"time to convert matrix: {t1-t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDZc7dVdCtZ0"
   },
   "source": [
    "### load the label encoder\n",
    "The labels must be in the same order as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_4DCbqRCtZ1"
   },
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.classes_ = np.load('SeqSeek-Classify-NN/dirty_neuron_encoder.npy')\n",
    "ohe_encoder = sklearn.preprocessing.LabelBinarizer()\n",
    "ohe_encoder.classes_ = np.load('SeqSeek-Classify-NN/dirty_neuron_encoder.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPsTjyZzCtZ7"
   },
   "source": [
    "### split the data into train, validation, and test\n",
    "we are only interested in the test data for this example.  In order to have a consistant train, val, test data split between the data, I take the md5 hash of the barcode, which is a large integer.  I look at the mod 100 of the barcode and take 80% of the data as training data 10% validation, and 10% testing.\n",
    "\n",
    "In order to use the data, I have to convert the sparse matrix into a sparse tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BTHLLfmoCtZ8",
    "outputId": "52195610-15ec-4f72-e4eb-0697ba7096c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': 22591, 'Test': 2810, 'Val': 2837}\n",
      "time to convert sparse matrix to sparse tensor for the Test data: 0.264 s\n"
     ]
    }
   ],
   "source": [
    "bchash = list( map(lambda bc: int(hashlib.md5(bc.encode()).hexdigest(),16)%100,lbl.cell_id))\n",
    "dataset = list( map(lambda hash: \"Train\" if hash<80 else (\"Val\" if hash < 90 else \"Test\"),bchash) )\n",
    "del bchash\n",
    "print(dict(Counter(dataset)))\n",
    "\n",
    "t0 = time.time()\n",
    "x_test  = make_log_maxn_tensor(dirty_neurons,dataset,\"Test\")\n",
    "t1 = time.time()\n",
    "print(f\"time to convert sparse matrix to sparse tensor for the Test data: {t1-t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzsnBb-ECtaE"
   },
   "source": [
    "Normally at prediction time, we would not have labels.  But since we have them, let's use them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRspEI7eCtaF"
   },
   "outputs": [],
   "source": [
    "y_test = lbl.final_cluster_assignment.values[ [x == \"Test\" for x in dataset] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUMpQq6GCtaK"
   },
   "source": [
    "we now have the testing data and are ready to run it through the model...\n",
    "\n",
    "# Step Three:\n",
    "load the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "CTj2dCV-CtaL",
    "outputId": "86ed4682-3af9-45c5-aecb-a8cfe163dfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               6507520   \n",
      "_________________________________________________________________\n",
      "cell_type (Dense)            (None, 70)                17990     \n",
      "=================================================================\n",
      "Total params: 6,525,510\n",
      "Trainable params: 6,525,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('SeqSeek-Classify-NN/neurons_doublets.model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "kaxQ9uKiCtaP",
    "outputId": "da6a2538-d25b-45d6-d0c5-88abe69805a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>called_class</th>\n",
       "      <th>predicted_type</th>\n",
       "      <th>called_type</th>\n",
       "      <th>probability</th>\n",
       "      <th>agree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.947911</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.964306</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.097964</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.957435</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Excit-01</td>\n",
       "      <td>Excit-01</td>\n",
       "      <td>0.538859</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>Excit-35</td>\n",
       "      <td>Excit-35</td>\n",
       "      <td>0.972334</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>Inhib-23</td>\n",
       "      <td>Inhib-23</td>\n",
       "      <td>0.951047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Excit-22</td>\n",
       "      <td>Excit-22</td>\n",
       "      <td>0.453575</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>Excit-35</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.799530</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>Inhib-09</td>\n",
       "      <td>Inhib-09</td>\n",
       "      <td>0.898915</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2810 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted_class  called_class  ... probability  agree\n",
       "0                  39            39  ...    0.947911   True\n",
       "1                  39            39  ...    0.964306   True\n",
       "2                  39            39  ...    0.097964   True\n",
       "3                  39            39  ...    0.957435   True\n",
       "4                   1             1  ...    0.538859   True\n",
       "...               ...           ...  ...         ...    ...\n",
       "2805               35            35  ...    0.972334   True\n",
       "2806               62            62  ...    0.951047   True\n",
       "2807               22            22  ...    0.453575   True\n",
       "2808               35            39  ...    0.799530  False\n",
       "2809               48            48  ...    0.898915   True\n",
       "\n",
       "[2810 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "cell_class = np.argmax(pred,axis=1)\n",
    "cell_type = label_encoder.inverse_transform(cell_class)\n",
    "called_class = label_encoder.transform(y_test)\n",
    "\n",
    "df = pd.DataFrame({\"predicted_class\":cell_class,\"called_class\":called_class,\"predicted_type\":cell_type,\"called_type\":y_test,\"probability\":np.max(pred,axis=1)} )\n",
    "df['agree'] = df.predicted_class == df.called_class\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of example_use_NN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
